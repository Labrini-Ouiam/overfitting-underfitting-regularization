# Overfitting, Underfitting & Regularization

Ce projet explore les concepts de **surd√©apprentissage (overfitting)**, **sous-apprentissage (underfitting)** et les techniques de **r√©gularisation L1 (LASSO)** et **L2 (Ridge)** √† travers un mod√®le de r√©gression polynomiale multivari√©e appliqu√© au dataset *Advertising*.

## üìå Objectif

- Impl√©menter la r√©gression polynomiale multivari√©e.
- Ajouter des termes de r√©gularisation L1 (LASSO) et L2 (Ridge).
- Comparer les performances du mod√®le (avec et sans r√©gularisation) sur les jeux d'entra√Ænement et de test.

## üóÉÔ∏è Dataset

Le jeu de donn√©es **Advertising** contient des informations sur les budgets publicitaires et les ventes correspondantes. Les variables ind√©pendantes comprennent :
- TV
- Radio
- Newspaper

La variable cible est :
- Sales

## üß† M√©thodes

1. R√©gression polynomiale multivari√©e (sans r√©gularisation)
2. R√©gression avec :
   - R√©gularisation L1 (LASSO)
   - R√©gularisation L2 (Ridge)
3. Comparaison des performances avec :
   - Mean Squared Error (MSE)
   - R¬≤ Score
   - Analyse via un tableau de comparaison

## üìä R√©sultats

Les r√©sultats sont pr√©sent√©s sous forme de tableau dans le notebook, montrant l'impact de chaque m√©thode de r√©gularisation sur les erreurs de pr√©diction sur les jeux **Train** et **Test**.

## üîß Technologies

- Python
- NumPy, Pandas
- Scikit-learn
- Matplotlib, Seaborn
- Jupyter Notebook

## üìÅ Fichier

- `overfitting-underfitting-regularization.ipynb` : Le notebook principal contenant le code, les visualisations, et les analyses.

## üë§ Auteur

**Labrini Ouiam**


---

> Pour toute remarque ou am√©lioration, n'h√©sitez pas √† cr√©er une issue ou un pull request !
